{"cells":[{"cell_type":"markdown","source":["# Feature Store taxi example notebook\n\nThis notebook illustrates the use of Feature Store to create a model that predicts NYC Yellow Taxi fares. It includes these steps:\n\n- Compute and write features.\n- Train a model using these features to predict fares.\n- Evaluate that model on a new batch of data using existing features, saved to Feature Store.\n\n## Requirements\n- Databricks Runtime for Machine Learning 8.3 or above. \n\n**Note:** This notebook is written to run with Databricks Runtime for Machine Learning 10.2 or above. If you are using Databricks Runtime for Machine Learning 10.1 or below, delete or comment out Cmd 19 and uncomment Cmd 20."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18113225-180e-4ca0-af30-458edb613222"}}},{"cell_type":"markdown","source":["<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_flow_v3.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e3a5fd7-8c2f-4573-9229-95b567fdeb2b"}}},{"cell_type":"markdown","source":["## Compute features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e36e088-6eb2-48be-a832-924522cfcaf2"}}},{"cell_type":"markdown","source":["#### Load the raw data used to compute features\n\nLoad the `nyc-taxi-tiny` dataset.  This was generated from the full [NYC Taxi Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) which can be found at `dbfs:/databricks-datasets/nyctaxi` by applying the following transformations:\n\n1. Apply a UDF to convert latitude and longitude coordinates into ZIP codes, and add a ZIP code column to the DataFrame.\n1. Subsample the dataset into a smaller dataset based on a date range query using the `.sample()` method of the Spark `DataFrame` API.\n1. Rename certain columns and drop unnecessary columns.\n\nIf you want to create this dataset from the raw data yourself, follow these steps:\n1. Run the Feature Store taxi example dataset notebook ([AWS](https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example-dataset.html)|[Azure](https://docs.microsoft.com/azure/databricks/_static/notebooks/machine-learning/feature-store-taxi-example-dataset.html)|[GCP](https://docs.gcp.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example-dataset.html)) to generate the Delta table.\n1. In this notebook, replace the following `spark.read.format(\"delta\").load(\"/databricks-datasets/nyctaxi-with-zipcodes/subsampled\")` with: `spark.read.table(\"feature_store_taxi_example.nyc_yellow_taxi_with_zips\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"180904ea-370c-4120-a839-71b961689f1b"}}},{"cell_type":"code","source":["raw_data = spark.read.format(\"delta\").load(\"/databricks-datasets/nyctaxi-with-zipcodes/subsampled\")\ndisplay(raw_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"990f4c53-af75-4928-876e-f27d09220475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["From the taxi fares transactional data, we will compute two groups of features based on trip pickup and drop off zip codes.\n\n#### Pickup features\n1. Count of trips (time window = 1 hour, sliding window = 15 minutes)\n1. Mean fare amount (time window = 1 hour, sliding window = 15 minutes)\n\n#### Drop off features\n1. Count of trips (time window = 30 minutes)\n1. Does trip end on the weekend (custom feature using python code)\n\n<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_computation_v5.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb85b000-cac4-4b1f-9935-ec21e4c01462"}}},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78660750-2ce7-46e5-a43f-a58a376eab3d"}}},{"cell_type":"code","source":["from databricks import feature_store\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import FloatType, IntegerType, StringType\nfrom pytz import timezone\n\n\n@udf(returnType=IntegerType())\ndef is_weekend(dt):\n    tz = \"America/New_York\"\n    return int(dt.astimezone(timezone(tz)).weekday() >= 5)  # 5 = Saturday, 6 = Sunday\n  \n@udf(returnType=StringType())  \ndef partition_id(dt):\n    # datetime -> \"YYYY-MM\"\n    return f\"{dt.year:04d}-{dt.month:02d}\"\n\n\ndef filter_df_by_ts(df, ts_column, start_date, end_date):\n    if ts_column and start_date:\n        df = df.filter(col(ts_column) >= start_date)\n    if ts_column and end_date:\n        df = df.filter(col(ts_column) < end_date)\n    return df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c57bb710-c4d2-44fd-bee1-612bc7a5b8c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Data scientist's custom code to compute features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddec3937-74dd-4dc2-8eee-939a441b9e6a"}}},{"cell_type":"code","source":["def pickup_features_fn(df, ts_column, start_date, end_date):\n    \"\"\"\n    Computes the pickup_features feature group.\n    To restrict features to a time range, pass in ts_column, start_date, and/or end_date as kwargs.\n    \"\"\"\n    df = filter_df_by_ts(\n        df, ts_column, start_date, end_date\n    )\n    pickupzip_features = (\n        df.groupBy(\n            \"pickup_zip\", window(\"tpep_pickup_datetime\", \"1 hour\", \"15 minutes\")\n        )  # 1 hour window, sliding every 15 minutes\n        .agg(\n            mean(\"fare_amount\").alias(\"mean_fare_window_1h_pickup_zip\"),\n            count(\"*\").alias(\"count_trips_window_1h_pickup_zip\"),\n        )\n        .select(\n            col(\"pickup_zip\").alias(\"zip\"),\n            unix_timestamp(col(\"window.end\")).alias(\"ts\").cast(IntegerType()),\n            partition_id(to_timestamp(col(\"window.end\"))).alias(\"yyyy_mm\"),\n            col(\"mean_fare_window_1h_pickup_zip\").cast(FloatType()),\n            col(\"count_trips_window_1h_pickup_zip\").cast(IntegerType()),\n        )\n    )\n    return pickupzip_features\n  \ndef dropoff_features_fn(df, ts_column, start_date, end_date):\n    \"\"\"\n    Computes the dropoff_features feature group.\n    To restrict features to a time range, pass in ts_column, start_date, and/or end_date as kwargs.\n    \"\"\"\n    df = filter_df_by_ts(\n        df,  ts_column, start_date, end_date\n    )\n    dropoffzip_features = (\n        df.groupBy(\"dropoff_zip\", window(\"tpep_dropoff_datetime\", \"30 minute\"))\n        .agg(count(\"*\").alias(\"count_trips_window_30m_dropoff_zip\"))\n        .select(\n            col(\"dropoff_zip\").alias(\"zip\"),\n            unix_timestamp(col(\"window.end\")).alias(\"ts\").cast(IntegerType()),\n            partition_id(to_timestamp(col(\"window.end\"))).alias(\"yyyy_mm\"),\n            col(\"count_trips_window_30m_dropoff_zip\").cast(IntegerType()),\n            is_weekend(col(\"window.end\")).alias(\"dropoff_is_weekend\"),\n        )\n    )\n    return dropoffzip_features  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07edf1e-7891-46d0-a15b-cdab573721ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime\n\npickup_features = pickup_features_fn(\n    raw_data, ts_column=\"tpep_pickup_datetime\", start_date=datetime(2016, 1, 1), end_date=datetime(2016, 1, 31)\n)\ndropoff_features = dropoff_features_fn(\n    raw_data, ts_column=\"tpep_dropoff_datetime\", start_date=datetime(2016, 1, 1), end_date=datetime(2016, 1, 31)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4956d48f-2af3-41eb-a283-de1bfbd34127"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(pickup_features)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d92f0ff1-6150-4097-857e-7056522e6d8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Use Feature Store library to create new feature tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3dd61dd-5baa-4af2-b036-6ebc5b443c02"}}},{"cell_type":"markdown","source":["First, create the database where the feature tables will be stored."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e606a8a1-1d99-4803-a314-c26ec8ced4d8"}}},{"cell_type":"code","source":["%sql \nCREATE DATABASE IF NOT EXISTS feature_store_taxi_example;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c41a3be3-5167-4fc2-94fb-be4f9f018348"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Next, create an instance of the Feature Store client."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a05988f-06fa-453f-b1aa-bcfc31dc7300"}}},{"cell_type":"code","source":["fs = feature_store.FeatureStoreClient()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"831b5164-adc0-4dd7-a4be-6109277ff4e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use either the `create_table` API (Databricks Runtime 10.2 ML or above) or the `create_feature_table` API (Databricks Runtime 10.1 ML or below) to define schema and unique ID keys. If the optional argument `df` (Databricks Runtime 10.2 ML or above) or `features_df` (Databricks Runtime 10.1 ML or below) is passed, the API also writes the data to Feature Store."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bca8d4dc-43de-4e10-89df-89cd87a4447d"}}},{"cell_type":"code","source":["# This cell uses an API introduced with Databricks Runtime 10.2 ML.\n# If your cluster is running Databricks Runtime 10.1 ML or below, skip or comment out this cell and uncomment and run Cmd 20.\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n\nfs.create_table(\n    name=\"feature_store_taxi_example.trip_pickup_features\",\n    primary_keys=[\"zip\", \"ts\"],\n    df=pickup_features,\n    partition_columns=\"yyyy_mm\",\n    description=\"Taxi Fares. Pickup Features\",\n)\nfs.create_table(\n    name=\"feature_store_taxi_example.trip_dropoff_features\",\n    primary_keys=[\"zip\", \"ts\"],\n    df=dropoff_features,\n    partition_columns=\"yyyy_mm\",\n    description=\"Taxi Fares. Dropoff Features\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30262aca-dae1-4540-b920-d64e65af7ecb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# To run this notebook with Databricks Runtime 10.1 ML or below, uncomment this cell.\n\n#spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n\n#fs.create_feature_table(\n#    name=\"feature_store_taxi_example.trip_pickup_features\",\n#    keys=[\"zip\", \"ts\"],\n#    features_df=pickup_features,\n#    partition_columns=\"yyyy_mm\",\n#    description=\"Taxi Fares. Pickup Features\",\n#)\n#fs.create_feature_table(\n#    name=\"feature_store_taxi_example.trip_dropoff_features\",\n#    keys=[\"zip\", \"ts\"],\n#    features_df=dropoff_features,\n#    partition_columns=\"yyyy_mm\",\n#    description=\"Taxi Fares. Dropoff Features\",\n#)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7fd9f49-0c03-42ce-b6ab-fce2909fe327"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Update features\n\nUse the `write_table` function to update the feature table values.\n\n<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_compute_and_write.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa182525-e752-401d-8565-b35da4490050"}}},{"cell_type":"code","source":["display(raw_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a23f9708-4ed6-4190-9c24-a26593db77b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Compute the pickup_features feature group.\npickup_features_df = pickup_features_fn(\n  df=raw_data,\n  ts_column=\"tpep_pickup_datetime\",\n  start_date=datetime(2016, 2, 1),\n  end_date=datetime(2016, 2, 29),\n)\n\n# Write the pickup features DataFrame to the feature store table\nfs.write_table(\n  name=\"feature_store_taxi_example.trip_pickup_features\",\n  df=pickup_features_df,\n  mode=\"merge\",\n)\n\n# Compute the dropoff_features feature group.\ndropoff_features_df = dropoff_features_fn(\n  df=raw_data,\n  ts_column=\"tpep_dropoff_datetime\",\n  start_date=datetime(2016, 2, 1),\n  end_date=datetime(2016, 2, 29),\n)\n\n# Write the dropoff features DataFrame to the feature store table\nfs.write_table(\n  name=\"feature_store_taxi_example.trip_dropoff_features\",\n  df=dropoff_features_df,\n  mode=\"merge\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccc8751a-6974-4b0c-9202-8c0b770466b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["When writing, both `merge` and `overwrite` modes are supported.\n\n    fs.write_table(\n      name=\"feature_store_taxi_example.trip_pickup_features\",\n      df=pickup_features_df,\n      mode=\"overwrite\",\n    )\n    \nData can also be streamed into Feature Store by passing a dataframe where `df.isStreaming` is set to `True`:\n\n    fs.write_table(\n      name=\"streaming_example.streaming_features\",\n      df=streaming_df,\n      mode=\"merge\",\n    )\n    \nYou can schedule a notebook to periodically update features using Databricks Jobs ([AWS](https://docs.databricks.com/jobs.html)|[Azure](https://docs.microsoft.com/azure/databricks/jobs)|[GCP](https://docs.gcp.databricks.com/jobs.html))."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f82ffe-9d60-4f33-8ae8-63d71e1a08a9"}}},{"cell_type":"markdown","source":["Analysts can interact with Feature Store using SQL, for example:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a2730e6-6ff9-406b-9551-208ec0aee521"}}},{"cell_type":"code","source":["%sql\nSELECT SUM(count_trips_window_30m_dropoff_zip) AS num_rides,\n       dropoff_is_weekend\nFROM   feature_store_taxi_example.trip_dropoff_features\nWHERE  dropoff_is_weekend IS NOT NULL\nGROUP  BY dropoff_is_weekend;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"189f3435-a29b-40ea-959c-b6f2d744dc1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Feature Search and Discovery"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49024838-72ae-44e6-a008-7b65b374335b"}}},{"cell_type":"markdown","source":["You can now discover your feature tables in the <a href=\"#feature-store/\" target=\"_blank\">Feature Store UI</a>.\n\n<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_flow_v3.png\"/>\n\nSearch by \"trip_pickup_features\" or \"trip_dropoff_features\" to view details such as table schema, metadata, data sources, producers, and online stores. \n\nYou can also edit the description for the feature table, or configure permissions for a feature table using the dropdown icon next to the feature table name. \n\nCheck the [Use the Feature Store UI\n](https://docs.databricks.com/applications/machine-learning/feature-store.html#use-the-feature-store-ui) documentation for more details."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a1b6ff3-bc3e-4504-ba03-b3d3d5ef5126"}}},{"cell_type":"markdown","source":["## Train a model\n\nThis section illustrates how to train a model using the pickup and dropoff features stored in Feature Store. It trains a LightGBM model to predict taxi fare."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18f3faaa-f31d-465c-aa13-fb79c37e2c02"}}},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b9b226-a94b-4489-be80-7ea39abfff61"}}},{"cell_type":"code","source":["from pyspark.sql import *\nfrom pyspark.sql.functions import current_timestamp\nfrom pyspark.sql.types import IntegerType\nimport math\nfrom datetime import timedelta\nimport mlflow.pyfunc\n\n\ndef rounded_unix_timestamp(dt, num_minutes=15):\n    \"\"\"\n    Ceilings datetime dt to interval num_minutes, then returns the unix timestamp.\n    \"\"\"\n    nsecs = dt.minute * 60 + dt.second + dt.microsecond * 1e-6\n    delta = math.ceil(nsecs / (60 * num_minutes)) * (60 * num_minutes) - nsecs\n    return int((dt + timedelta(seconds=delta)).timestamp())\n\n\nrounded_unix_timestamp_udf = udf(rounded_unix_timestamp, IntegerType())\n\n\ndef rounded_taxi_data(taxi_data_df):\n    # Round the taxi data timestamp to 15 and 30 minute intervals so we can join with the pickup and dropoff features\n    # respectively.\n    taxi_data_df = (\n        taxi_data_df.withColumn(\n            \"rounded_pickup_datetime\",\n            rounded_unix_timestamp_udf(taxi_data_df[\"tpep_pickup_datetime\"], lit(15)),\n        )\n        .withColumn(\n            \"rounded_dropoff_datetime\",\n            rounded_unix_timestamp_udf(taxi_data_df[\"tpep_dropoff_datetime\"], lit(30)),\n        )\n        .drop(\"tpep_pickup_datetime\")\n        .drop(\"tpep_dropoff_datetime\")\n    )\n    taxi_data_df.createOrReplaceTempView(\"taxi_data\")\n    return taxi_data_df\n  \ndef get_latest_model_version(model_name):\n  latest_version = 1\n  mlflow_client = MlflowClient()\n  for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n    version_int = int(mv.version)\n    if version_int > latest_version:\n      latest_version = version_int\n  return latest_version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4653bfb1-5ea0-4751-99f0-ad00482cf080"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Read taxi data for training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907afbdb-c7ed-4aaf-a6d9-296ade7ef743"}}},{"cell_type":"code","source":["taxi_data = rounded_taxi_data(raw_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d33266c6-12d2-4a5c-909c-82d512faed5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Understanding how a training dataset is created\n\nIn order to train a model, you need to create a training dataset that is used to train the model.  The training dataset is comprised of:\n\n1. Raw input data\n1. Features from the feature store\n\nThe raw input data is needed because it contains:\n\n1. Primary keys used to join with features.\n1. Raw features like `trip_distance` that are not in the feature store.\n1. Prediction targets like `fare` that are required for model training.\n\nHere's a visual overview that shows the raw input data being combined with the features in the Feature Store to produce the training dataset:\n\n<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_feature_lookup.png\"/>\n\nThese concepts are described further in the Creating a Training Dataset documentation ([AWS](https://docs.databricks.com/applications/machine-learning/feature-store.html#create-a-training-dataset)|[Azure](https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/feature-store#create-a-training-dataset)|[GCP](https://docs.gcp.databricks.com/applications/machine-learning/feature-store.html#create-a-training-dataset)).\n\nThe next cell loads features from Feature Store for model training by creating a `FeatureLookup` for each needed feature."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"574f9128-a5be-42b7-8d64-3fa7f2e0c721"}}},{"cell_type":"code","source":["# If you are running Databricks Runtime for Machine Learning 9.1 or above, you can also use the alternate API shown in Cmd 35 to create the FeatureLookups.\n\nfrom databricks.feature_store import FeatureLookup\nimport mlflow\n\npickup_features_table = \"feature_store_taxi_example.trip_pickup_features\"\ndropoff_features_table = \"feature_store_taxi_example.trip_dropoff_features\"\n\npickup_feature_lookups = [\n    FeatureLookup( \n      table_name = pickup_features_table,\n      feature_name = \"mean_fare_window_1h_pickup_zip\",\n      lookup_key = [\"pickup_zip\", \"rounded_pickup_datetime\"],\n    ),\n    FeatureLookup( \n      table_name = pickup_features_table,\n      feature_name = \"count_trips_window_1h_pickup_zip\",\n      lookup_key = [\"pickup_zip\", \"rounded_pickup_datetime\"],\n    ),\n]\n\ndropoff_feature_lookups = [\n    FeatureLookup( \n      table_name = dropoff_features_table,\n      feature_name = \"count_trips_window_30m_dropoff_zip\",\n      lookup_key = [\"dropoff_zip\", \"rounded_dropoff_datetime\"],\n    ),\n    FeatureLookup( \n      table_name = dropoff_features_table,\n      feature_name = \"dropoff_is_weekend\",\n      lookup_key = [\"dropoff_zip\", \"rounded_dropoff_datetime\"],\n    ),\n]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb7bb017-63cd-46d7-a77a-f60d6501ad75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# If you are running Databricks Runtime for Machine Learning 9.1 or above, you can uncomment the code in this cell and use it instead of the code in Cmd 34.\n\n#from databricks.feature_store import FeatureLookup\n#import mlflow\n\n#pickup_features_table = \"feature_store_taxi_example.trip_pickup_features\"\n#dropoff_features_table = \"feature_store_taxi_example.trip_dropoff_features\"\n\n#pickup_feature_lookups = [\n#    FeatureLookup( \n#      table_name = pickup_features_table,\n#      feature_names = [\"mean_fare_window_1h_pickup_zip\", \"count_trips_window_1h_pickup_zip\"],\n#      lookup_key = [\"pickup_zip\", \"rounded_pickup_datetime\"],\n#    ),\n#]\n\n#dropoff_feature_lookups = [\n#    FeatureLookup( \n#      table_name = dropoff_features_table,\n#      feature_names = [\"count_trips_window_30m_dropoff_zip\", \"dropoff_is_weekend\"],\n#      lookup_key = [\"dropoff_zip\", \"rounded_dropoff_datetime\"],\n#    ),\n#]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41e07a79-fd72-4f3c-a019-24a5bada1c32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Create a Training Dataset\n\nWhen `fs.create_training_set(..)` is invoked below, the following steps will happen:\n\n1. A `TrainingSet` object will be created, which will select specific features from Feature Store to use in training your model. Each feature is specified by the `FeatureLookup`'s created above. \n\n1. Features are joined with the raw input data according to each `FeatureLookup`'s `lookup_key`.\n\nThe `TrainingSet` is then transformed into a DataFrame to train on. This DataFrame includes the columns of taxi_data, as well as the features specified in the `FeatureLookups`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b399de7d-b5c2-45b5-a8ed-50344ca73012"}}},{"cell_type":"code","source":["# End any existing runs (in the case this notebook is being run for a second time)\nmlflow.end_run()\n\n# Start an mlflow run, which is needed for the feature store to log the model\nmlflow.start_run() \n\n# Since the rounded timestamp columns would likely cause the model to overfit the data \n# unless additional feature engineering was performed, exclude them to avoid training on them.\nexclude_columns = [\"rounded_pickup_datetime\", \"rounded_dropoff_datetime\"]\n\n# Create the training set that includes the raw input data merged with corresponding features from both feature tables\ntraining_set = fs.create_training_set(\n  taxi_data,\n  feature_lookups = pickup_feature_lookups + dropoff_feature_lookups,\n  label = \"fare_amount\",\n  exclude_columns = exclude_columns\n)\n\n# Load the TrainingSet into a dataframe which can be passed into sklearn for training a model\ntraining_df = training_set.load_df()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2b39990-4858-46a6-9b57-c9ed97ad381b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Display the training dataframe, and note that it contains both the raw input data and the features from the Feature Store, like `dropoff_is_weekend`\ndisplay(training_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b47c7ad5-00fa-420b-bc01-7224760e8008"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Train a LightGBM model on the data returned by `TrainingSet.to_df`, then log the model with `FeatureStoreClient.log_model`. The model will be packaged with feature metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebedacd8-f193-4671-81b1-f5cf7e4cdf6d"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nfrom mlflow.tracking import MlflowClient\nimport lightgbm as lgb\nimport mlflow.lightgbm\nfrom mlflow.models.signature import infer_signature\n\nfeatures_and_label = training_df.columns\n\n# Collect data into a Pandas array for training\ndata = training_df.toPandas()[features_and_label]\n\ntrain, test = train_test_split(data, random_state=123)\nX_train = train.drop([\"fare_amount\"], axis=1)\nX_test = test.drop([\"fare_amount\"], axis=1)\ny_train = train.fare_amount\ny_test = test.fare_amount\n\nmlflow.lightgbm.autolog()\ntrain_lgb_dataset = lgb.Dataset(X_train, label=y_train.values)\ntest_lgb_dataset = lgb.Dataset(X_test, label=y_test.values)\n\nparam = {\"num_leaves\": 32, \"objective\": \"regression\", \"metric\": \"rmse\"}\nnum_rounds = 100\n\n# Train a lightGBM model\nmodel = lgb.train(\n  param, train_lgb_dataset, num_rounds\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a83d5c43-5575-43b3-a1c6-95706dfbcc0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Log the trained model with MLflow and package it with feature lookup information. \nfs.log_model(\n  model,\n  artifact_path=\"model_packaged\",\n  flavor=mlflow.lightgbm,\n  training_set=training_set,\n  registered_model_name=\"taxi_example_fare_packaged\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c942585-9e1f-4258-a8dd-ab2746c26101"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Scoring: Batch Inference"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdd10ec1-545d-4f00-9125-d7f0db54645c"}}},{"cell_type":"markdown","source":["Suppose another data scientist now wants to apply this model to a different batch of data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c0b5ffe-4cdf-4c48-85ca-8f0e4066418b"}}},{"cell_type":"code","source":["new_taxi_data = rounded_taxi_data(raw_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab544257-e3c5-47c4-b14e-b9cedac43f09"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Display the data to use for inference, reordered to highlight the `fare_amount` column, which is the prediction target."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d45c396f-3a6b-44fd-ac28-b0d2bc5cfef1"}}},{"cell_type":"code","source":["cols = ['fare_amount', 'trip_distance', 'pickup_zip', 'dropoff_zip', 'rounded_pickup_datetime', 'rounded_dropoff_datetime']\nnew_taxi_data_reordered = new_taxi_data.select(cols)\ndisplay(new_taxi_data_reordered)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24d88606-4591-48d8-a600-ac8e57dc6f82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use the `score_batch` API to evaluate the model on the batch of data, retrieving needed features from FeatureStore."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d1b1882-ca90-4588-811d-09891cb60643"}}},{"cell_type":"code","source":["# Get the model URI\nlatest_model_version = get_latest_model_version(\"taxi_example_fare_packaged\")\nmodel_uri = f\"models:/taxi_example_fare_packaged/{latest_model_version}\"\n\n# Call score_batch to get the predictions from the model\nwith_predictions = fs.score_batch(model_uri, new_taxi_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03e899e0-8a63-43ef-b8ba-3585c159de0d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/taxi_example_score_batch.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3e1c867-c3a1-4b1d-8183-bea6e9698ad8"}}},{"cell_type":"markdown","source":["### View the taxi fare predictions\n\nThis code reorders the columns to show the taxi fare predictions in the first column.  Note that the `predicted_fare_amount` roughly lines up with the actual `fare_amount`, although more data and feature engineering would be required to improve the model accuracy."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e71b3263-efcc-4667-a748-78ae560e1edc"}}},{"cell_type":"code","source":["import pyspark.sql.functions as func\n\ncols = ['prediction', 'fare_amount', 'trip_distance', 'pickup_zip', 'dropoff_zip', \n        'rounded_pickup_datetime', 'rounded_dropoff_datetime', 'mean_fare_window_1h_pickup_zip', \n        'count_trips_window_1h_pickup_zip', 'count_trips_window_30m_dropoff_zip', 'dropoff_is_weekend']\n\nwith_predictions_reordered = (\n    with_predictions.select(\n        cols,\n    )\n    .withColumnRenamed(\n        \"prediction\",\n        \"predicted_fare_amount\",\n    )\n    .withColumn(\n      \"predicted_fare_amount\",\n      func.round(\"predicted_fare_amount\", 2),\n    )\n)\n\ndisplay(with_predictions_reordered)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fcfb2a4-2c0b-450c-bfc4-3de07dec5a64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Next steps\n\n1. Explore the feature tables created in this example in the <a href=\"#feature-store\">Feature Store UI</a>.\n1. Adapt this notebook to your own data and create your own feature tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6750dae-a99e-462b-8624-78b61fdba8a0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Feature Store Taxi example notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3919844516505155}},"nbformat":4,"nbformat_minor":0}
